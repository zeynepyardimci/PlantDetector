{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "s0y1esU0o109"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets timm accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29K6UtAPpe3i"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, ConvNextForImageClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "model_ckpt = \"facebook/convnext-base-224\"  # veya convnext-small, convnext-tiny\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "model = ConvNextForImageClassification.from_pretrained(model_ckpt, num_labels=9,ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOT_jgrXqXYG"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "43OI_m7kszO6"
   },
   "outputs": [],
   "source": [
    "!pip install pyheif pillow-heif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xok2l9RyfIT"
   },
   "outputs": [],
   "source": [
    "pip install filetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5_hOvFtos3aR"
   },
   "outputs": [],
   "source": [
    "import filetype\n",
    "import os\n",
    "\n",
    "def add_extension_with_filetype(file_path):\n",
    "    if os.path.splitext(file_path)[1] == '':\n",
    "        kind = filetype.guess(file_path)\n",
    "        if kind:\n",
    "            new_file = file_path + '.' + kind.extension\n",
    "            os.rename(file_path, new_file)\n",
    "            print(f\"Uzantı eklendi: {file_path} -> {new_file}\")\n",
    "            return new_file\n",
    "        else:\n",
    "            print(f\"Dosya tipi belirlenemedi: {file_path}\")\n",
    "            return file_path\n",
    "    else:\n",
    "        return file_path\n",
    "\n",
    "# Klasörü dolaşalım\n",
    "folder = '/content/drive/MyDrive/Raw'\n",
    "\n",
    "for root, _, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(root, file)\n",
    "        add_extension_with_filetype(full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZYbe3Rnjy6eG"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade datasets fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXBga6fgqbmF"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dir = \"/content/drive/MyDrive/Raw\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ConvNeXt için 224x224 input boyutu\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f\"Toplam örnek sayısı: {len(dataset)}\")\n",
    "print(f\"Etiketler: {dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmiNxUlZznoR"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "2Q1LYtfb0nM9"
   },
   "outputs": [],
   "source": [
    "from transformers import ConvNextForImageClassification, ConvNextFeatureExtractor\n",
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Model ve feature extractor'ı indir\n",
    "model_name = \"facebook/convnext-tiny-224\"\n",
    "\n",
    "feature_extractor = ConvNextFeatureExtractor.from_pretrained(model_name)\n",
    "model = ConvNextForImageClassification.from_pretrained(model_name)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLdNpBSG0zO9"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# Hugging Face'den model yükleniyor\n",
    "model_ckpt = \"facebook/convnext-tiny-224\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_ckpt)\n",
    "\n",
    "# Preprocessing için dönüşüm (transform)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ConvNext 224x224 bekliyor\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=processor.image_mean, std=processor.image_std)\n",
    "])\n",
    "\n",
    "# Dataset yükleme\n",
    "dataset = ImageFolder(root=\"/content/drive/MyDrive/Raw\", transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGMtj4P-1-VZ"
   },
   "outputs": [],
   "source": [
    "# Örnek görüntü ve label kontrolü\n",
    "img, label = dataset[0]\n",
    "print(img.shape)  # torch.Size([3, 224, 224])\n",
    "print(label)      # 0 gibi sınıf indexi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZCIHSfq2FOB"
   },
   "source": [
    "modeli hazırla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJqA8lUl3wcu"
   },
   "outputs": [],
   "source": [
    "class_names = ['1. Potassium Deficiency', '2. Manganese Deficiency', '3. Magnesium Deficiency', '4. Black Scorch', '5. Leaf Spots', '6. Fusarium Wilt', '7. Rachis Blight', '8. Parlatoria Blanchardi', '9. Healthy sample']\n",
    "label2id = {label: idx for idx, label in enumerate(class_names)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkJjIIaX3BWu"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"facebook/convnext-base-384\",\n",
    "    num_labels=9,\n",
    "    label2id = {label: idx for idx, label in enumerate(dataset.class_to_idx)},\n",
    "    id2label = {idx: label for label, idx in dataset.class_to_idx.items()},\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QyxHmWu2HTN"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    \"facebook/convnext-base-384\",\n",
    "    num_labels=9,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvNLiymh6mdu"
   },
   "source": [
    "PyTorch ImageFolder → Hugging Face Dataset dönüşümü\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLJ1EdqYPnOR"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from datasets import Dataset, Features, ClassLabel\n",
    "from datasets.features import Image as HFDatasetImage\n",
    "from PIL import Image as PILImage\n",
    "from datasets import Value\n",
    "\n",
    "data_dir = \"/content/drive/MyDrive/Raw\"\n",
    "\n",
    "dataset = ImageFolder(root=data_dir, transform=None)\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for img_path, label in dataset.samples:\n",
    "    image_paths.append(img_path)\n",
    "    labels.append(label)\n",
    "\n",
    "dict_ds = {\n",
    "    \"image_path\": image_paths,\n",
    "    \"label\": labels\n",
    "}\n",
    "\n",
    "features = Features({\n",
    "    \"image_path\": Value(\"string\"),  # Burada dosya yolları\n",
    "    \"label\": ClassLabel(names=list(set(labels)))\n",
    "})\n",
    "\n",
    "\n",
    "hf_dataset = Dataset.from_dict(dict_ds, features=features)\n",
    "\n",
    "def safe_load_image_batch(example_batch):\n",
    "    images = []\n",
    "    for path in example_batch[\"image_path\"]:\n",
    "        try:\n",
    "            img = PILImage.open(path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Hata dosyada: {path} -> {e}\")\n",
    "            img = None\n",
    "        images.append(img)\n",
    "    return {\"image\": images}\n",
    "\n",
    "hf_dataset = hf_dataset.map(\n",
    "    safe_load_image_batch,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    desc=\"Görüntüleri yükle\"\n",
    ")\n",
    "\n",
    "def filter_none_images(example):\n",
    "    return example[\"image\"] is not None\n",
    "\n",
    "hf_dataset = hf_dataset.filter(filter_none_images)\n",
    "\n",
    "print(f\"Temizlenmiş dataset hazır, toplam örnek sayısı: {len(hf_dataset)}\")\n",
    "print(hf_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSLKqo08R-98"
   },
   "outputs": [],
   "source": [
    "print(hf_dataset.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Z1gMlOEPR1BB"
   },
   "outputs": [],
   "source": [
    "def filter_none_images(example):\n",
    "    return example[\"image\"] is not None\n",
    "\n",
    "hf_dataset = hf_dataset.filter(filter_none_images)\n",
    "print(f\"Temizlenmiş dataset, toplam örnek sayısı: {len(hf_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o5paXqt4Xch"
   },
   "source": [
    "Eğitim Kodu (Trainer ile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8_8eQViEJB6e"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "JX6vrUx1Mc5C"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlmIlVkc4YIl"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, AutoImageProcessor, ConvNextForImageClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torchvision.datasets import ImageFolder\n",
    "from datasets import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# 1. Image processor ve model seçimi\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/convnext-tiny-224\")\n",
    "\n",
    "# Doğru sınıf sayısı ile model tanımı\n",
    "data_dir = \"/content/drive/MyDrive/Raw\"\n",
    "torch_dataset = ImageFolder(root=data_dir)\n",
    "num_classes = len(torch_dataset.classes)\n",
    "\n",
    "model = ConvNextForImageClassification.from_pretrained(\n",
    "    \"facebook/convnext-tiny-224\",\n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# 2. Metrik fonksiyonu\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# 3. Dataset yükle\n",
    "image_paths = [path for path, _ in torch_dataset.samples]\n",
    "labels = [label for _, label in torch_dataset.samples]\n",
    "dict_ds = {\"image_path\": image_paths, \"label\": labels}\n",
    "hf_dataset = Dataset.from_dict(dict_ds)\n",
    "\n",
    "# 4. Görselleri yükle (önceden resize edip tensor'a çeviriyoruz)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_images(example_batch):\n",
    "    images = []\n",
    "    for path in example_batch[\"image_path\"]:\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = image_transform(img)\n",
    "        images.append(img)\n",
    "    return {\"pixel_values\": images}\n",
    "\n",
    "hf_dataset = hf_dataset.map(load_images, batched=True, num_proc=1)\n",
    "\n",
    "# 5. Train/test ayır\n",
    "split_dataset = hf_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "# 6. transform fonksiyonu\n",
    "def transform(example_batch):\n",
    "    inputs = processor(images=example_batch[\"pixel_values\"], return_tensors=\"pt\")\n",
    "    example_batch[\"pixel_values\"] = list(inputs[\"pixel_values\"])\n",
    "    example_batch[\"labels\"] = example_batch[\"label\"]\n",
    "    return example_batch\n",
    "\n",
    "# Her iki subset'te ayrı ayrı transform uygula\n",
    "train_dataset = split_dataset[\"train\"].map(transform, batched=True)\n",
    "test_dataset = split_dataset[\"test\"].map(transform, batched=True)\n",
    "\n",
    "encoded_dataset = split_dataset.map(transform, batched=True)\n",
    "\n",
    "# 7. Eğitim argümanları\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./convnext-results\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True  # Colab için daha az bellek kullanımı\n",
    ")\n",
    "#data collator\n",
    "def data_collator(features):\n",
    "    pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
    "    labels = torch.tensor([f[\"labels\"] for f in features])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# 8. Trainer nesnesi (custom data collator ile!)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 9. Eğitimi başlat\n",
    "train_output = trainer.train()\n",
    "\n",
    "# 10. Metrikleri kaydet\n",
    "with open(\"train_metrics.json\", \"w\") as f:\n",
    "    json.dump(train_output.metrics, f, indent=4)\n",
    "\n",
    "eval_metrics = trainer.evaluate()\n",
    "with open(\"eval_metrics.json\", \"w\") as f:\n",
    "    json.dump(eval_metrics, f, indent=4)\n",
    "\n",
    "# 11. Modeli kaydet\n",
    "trainer.save_model(\"./convnext-model\")\n",
    "processor.save_pretrained(\"./convnext-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ELMlC5JyKOrf"
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets torchvision scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VN3YmSJFLNM3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir())  # Bulunduğun dizindeki klasör ve dosyaları göster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "W5tGc1J5MAiD"
   },
   "outputs": [],
   "source": [
    "pip install --upgrade fsspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kX8RLoZhKTM3"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root=\"/content/drive/MyDrive/Raw\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "print(dataset.class_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO-i5U_bMOYj"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copytree(\"/content/drive/MyDrive/Raw\", \"/tmp/Raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2ym_NOONE4G"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from transformers import ConvNextForImageClassification, ConvNextImageProcessor, Trainer, TrainingArguments\n",
    "\n",
    "# 1. Veri yolu\n",
    "data_dir = \"/content/drive/MyDrive/Raw\" \n",
    "\n",
    "# 2. Görsel transformasyonları\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 3. Dataset ve split\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 4. Processor ve model\n",
    "processor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-tiny-224\")\n",
    "model = ConvNextForImageClassification.from_pretrained(\n",
    "    \"facebook/convnext-tiny-224\",\n",
    "    num_labels=len(dataset.classes),\n",
    "    id2label={i: c for i, c in enumerate(dataset.classes)},\n",
    "    label2id={c: i for i, c in enumerate(dataset.classes)},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# 5. Collate function — Trainer ile DataLoader\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    # processor, pil image listesi alır ve tensor döner\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    labels = torch.tensor(labels)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# 6. Trainer'ın içine DataLoader koymuyoruz direkt Dataset veriyoruz,\n",
    "# Trainer kendi içinde batch ve collate yapıyor\n",
    "# Bu yüzden train_ds ve val_ds kullanacağız, sadece collate_fn'u Trainer'a veriyoruz\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# 7. Eğitimi başlat\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMuUZLLdPC4D"
   },
   "source": [
    "yeni bir deneme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Kso3EHpwR_IX"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ebyDe72OSNNA"
   },
   "outputs": [],
   "source": [
    "pip install -U transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTgbDN5Tn5Ws"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18441160,
     "status": "ok",
     "timestamp": 1752930973275,
     "user": {
      "displayName": "ZEYNEP YARDIMCI",
      "userId": "08243208065676408191"
     },
     "user_tz": -180
    },
    "id": "8aRr5o9mRW03",
    "outputId": "1688fd79-164c-4ff9-8472-c849a8d6722b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at facebook/convnext-tiny-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([9, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2100' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2100/7500 5:06:02 < 13:07:42, 0.11 it/s, Epoch 28/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.954400</td>\n",
       "      <td>1.982176</td>\n",
       "      <td>0.258278</td>\n",
       "      <td>0.169308</td>\n",
       "      <td>0.126020</td>\n",
       "      <td>0.258278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.823200</td>\n",
       "      <td>1.843424</td>\n",
       "      <td>0.324503</td>\n",
       "      <td>0.246932</td>\n",
       "      <td>0.255279</td>\n",
       "      <td>0.324503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.691800</td>\n",
       "      <td>1.730693</td>\n",
       "      <td>0.397351</td>\n",
       "      <td>0.351075</td>\n",
       "      <td>0.469063</td>\n",
       "      <td>0.397351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.332300</td>\n",
       "      <td>1.675082</td>\n",
       "      <td>0.390728</td>\n",
       "      <td>0.346048</td>\n",
       "      <td>0.442424</td>\n",
       "      <td>0.390728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.164800</td>\n",
       "      <td>1.509140</td>\n",
       "      <td>0.536424</td>\n",
       "      <td>0.524598</td>\n",
       "      <td>0.554514</td>\n",
       "      <td>0.536424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.976100</td>\n",
       "      <td>1.445271</td>\n",
       "      <td>0.523179</td>\n",
       "      <td>0.494602</td>\n",
       "      <td>0.533465</td>\n",
       "      <td>0.523179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.714200</td>\n",
       "      <td>1.441249</td>\n",
       "      <td>0.509934</td>\n",
       "      <td>0.508364</td>\n",
       "      <td>0.554134</td>\n",
       "      <td>0.509934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.483000</td>\n",
       "      <td>1.287554</td>\n",
       "      <td>0.589404</td>\n",
       "      <td>0.569601</td>\n",
       "      <td>0.561559</td>\n",
       "      <td>0.589404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>1.291901</td>\n",
       "      <td>0.576159</td>\n",
       "      <td>0.566740</td>\n",
       "      <td>0.610091</td>\n",
       "      <td>0.576159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.240900</td>\n",
       "      <td>1.386017</td>\n",
       "      <td>0.562914</td>\n",
       "      <td>0.559899</td>\n",
       "      <td>0.632889</td>\n",
       "      <td>0.562914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>1.253273</td>\n",
       "      <td>0.589404</td>\n",
       "      <td>0.588052</td>\n",
       "      <td>0.601385</td>\n",
       "      <td>0.589404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>1.328614</td>\n",
       "      <td>0.622517</td>\n",
       "      <td>0.615744</td>\n",
       "      <td>0.633198</td>\n",
       "      <td>0.622517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>1.378038</td>\n",
       "      <td>0.596026</td>\n",
       "      <td>0.591999</td>\n",
       "      <td>0.626205</td>\n",
       "      <td>0.596026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.164600</td>\n",
       "      <td>1.378275</td>\n",
       "      <td>0.629139</td>\n",
       "      <td>0.633354</td>\n",
       "      <td>0.657620</td>\n",
       "      <td>0.629139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>1.488706</td>\n",
       "      <td>0.602649</td>\n",
       "      <td>0.615559</td>\n",
       "      <td>0.666305</td>\n",
       "      <td>0.602649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>1.535290</td>\n",
       "      <td>0.602649</td>\n",
       "      <td>0.604298</td>\n",
       "      <td>0.622276</td>\n",
       "      <td>0.602649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>1.607879</td>\n",
       "      <td>0.615894</td>\n",
       "      <td>0.611449</td>\n",
       "      <td>0.650281</td>\n",
       "      <td>0.615894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>1.718309</td>\n",
       "      <td>0.596026</td>\n",
       "      <td>0.605438</td>\n",
       "      <td>0.654935</td>\n",
       "      <td>0.596026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>1.899742</td>\n",
       "      <td>0.589404</td>\n",
       "      <td>0.590149</td>\n",
       "      <td>0.644471</td>\n",
       "      <td>0.589404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>1.770063</td>\n",
       "      <td>0.556291</td>\n",
       "      <td>0.560655</td>\n",
       "      <td>0.599940</td>\n",
       "      <td>0.556291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>1.808078</td>\n",
       "      <td>0.609272</td>\n",
       "      <td>0.617073</td>\n",
       "      <td>0.640881</td>\n",
       "      <td>0.609272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/convnext-model3/preprocessor_config.json']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    ConvNextForImageClassification,\n",
    "    ConvNextImageProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "# 0. Google Drive'ı mount et (çalıştırdığında Drive'a erişim vereceksin)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 1. Veri yolu\n",
    "data_dir = \"/content/drive/MyDrive/Raw\"  \n",
    "\n",
    "# 2. Görsel transformasyonları\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 3. Dataset ve split\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 4. Processor ve model\n",
    "processor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-tiny-224\")\n",
    "model = ConvNextForImageClassification.from_pretrained(\n",
    "    \"facebook/convnext-tiny-224\",\n",
    "    num_labels=len(dataset.classes),\n",
    "    id2label={i: c for i, c in enumerate(dataset.classes)},\n",
    "    label2id={c: i for i, c in enumerate(dataset.classes)},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# 5. Collate function (batch olarak işleme için)\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    labels = torch.tensor(labels)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# 6. Metrik fonksiyonu (accuracy, f1, precision, recall)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# 7. TrainingArguments (Kaydetme, logging ve değerlendirme ayarları)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/convnext-results3\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    # Early stopping için gerekenler\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",  \n",
    "    eval_strategy=\"steps\",       # 'epoch' da olabilir ama steps daha iyi erken durdurmada\n",
    "    eval_steps=100,                    # Kaç adımda bir validation yapılacağı\n",
    ")\n",
    "\n",
    "# 8. Trainer nesnesi\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,  \n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=7)],  # patience: kaç defa artmazsa dursun\n",
    ")\n",
    "\n",
    "# 9. Eğitimi başlat\n",
    "train_output = trainer.train()\n",
    "\n",
    "# 10. Metrikleri kaydet (Drive içine)\n",
    "with open(\"/content/drive/MyDrive/train_metrics3.json\", \"w\") as f:\n",
    "    json.dump(train_output.metrics, f, indent=4)\n",
    "\n",
    "eval_metrics = trainer.evaluate()\n",
    "with open(\"/content/drive/MyDrive/eval_metrics3.json\", \"w\") as f:\n",
    "    json.dump(eval_metrics, f, indent=4)\n",
    "\n",
    "# 11. Model ve processor'ü kaydet (Drive içine)\n",
    "trainer.save_model(\"/content/drive/MyDrive/convnext-model3\")\n",
    "processor.save_pretrained(\"/content/drive/MyDrive/convnext-model3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv8uiMIOqg0S"
   },
   "source": [
    "convnext-base-224 modeli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "executionInfo": {
     "elapsed": 1736265,
     "status": "ok",
     "timestamp": 1753019888842,
     "user": {
      "displayName": "ZEYNEP YARDIMCI",
      "userId": "08243208065676408191"
     },
     "user_tz": -180
    },
    "id": "JoGgJ5zgqfL5",
    "outputId": "704ed895-5a20-4164-a23e-cd2c45b8db18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ConvNextForImageClassification were not initialized from the model checkpoint at facebook/convnext-base-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 1024]) in the checkpoint and torch.Size([9, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([9]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Didn't manage to set back the RNG states of the CUDA because of the following error:\n",
      " 'cuda'\n",
      "This won't yield the same results as if the training had not been interrupted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3700' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3700/7500 27:42 < 1:57:13, 0.54 it/s, Epoch 49/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.167300</td>\n",
       "      <td>0.156538</td>\n",
       "      <td>0.960265</td>\n",
       "      <td>0.961462</td>\n",
       "      <td>0.966235</td>\n",
       "      <td>0.960265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.146740</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.974817</td>\n",
       "      <td>0.979296</td>\n",
       "      <td>0.973510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.234100</td>\n",
       "      <td>0.953642</td>\n",
       "      <td>0.954394</td>\n",
       "      <td>0.961084</td>\n",
       "      <td>0.953642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.063700</td>\n",
       "      <td>0.309545</td>\n",
       "      <td>0.927152</td>\n",
       "      <td>0.927244</td>\n",
       "      <td>0.930778</td>\n",
       "      <td>0.927152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.310953</td>\n",
       "      <td>0.927152</td>\n",
       "      <td>0.927388</td>\n",
       "      <td>0.931528</td>\n",
       "      <td>0.927152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.322303</td>\n",
       "      <td>0.927152</td>\n",
       "      <td>0.927449</td>\n",
       "      <td>0.931734</td>\n",
       "      <td>0.927152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.276023</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.934355</td>\n",
       "      <td>0.939307</td>\n",
       "      <td>0.933775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.322993</td>\n",
       "      <td>0.940397</td>\n",
       "      <td>0.941033</td>\n",
       "      <td>0.946114</td>\n",
       "      <td>0.940397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.285741</td>\n",
       "      <td>0.933775</td>\n",
       "      <td>0.934355</td>\n",
       "      <td>0.939307</td>\n",
       "      <td>0.933775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['/content/drive/MyDrive/convnext-model-base/preprocessor_config.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    ConvNextForImageClassification,\n",
    "    ConvNextImageProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import json\n",
    "from google.colab import drive\n",
    "\n",
    "# 0. Google Drive'ı mount et (Drive erişimi için)\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 1. Veri yolu\n",
    "data_dir = \"/content/drive/MyDrive/Raw\"  \n",
    "\n",
    "# 2. Görsel transformasyonları\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 3. Dataset ve split\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 4. Processor ve model (ConvNext Base versiyonu)\n",
    "processor = ConvNextImageProcessor.from_pretrained(\"facebook/convnext-base-224\")\n",
    "model = ConvNextForImageClassification.from_pretrained(\n",
    "    \"facebook/convnext-base-224\",\n",
    "    num_labels=len(dataset.classes),\n",
    "    id2label={i: c for i, c in enumerate(dataset.classes)},\n",
    "    label2id={c: i for i, c in enumerate(dataset.classes)},\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# 5. Collate function (batch olarak işleme için)\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    labels = torch.tensor(labels)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# 6. Metrik fonksiyonu (accuracy, f1, precision, recall)\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# 7. TrainingArguments (Kaydetme, logging ve değerlendirme ayarları)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/convnext-results-base\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,                   # GPU varsa aç, yoksa False yap\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    ")\n",
    "\n",
    "# 8. Trainer nesnesi\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=7)],\n",
    ")\n",
    "\n",
    "# 9. Eğitimi başlat\n",
    "train_output = trainer.train(resume_from_checkpoint=\"/content/drive/MyDrive/convnext-results-base/checkpoint-2800\")\n",
    "\n",
    "# 10. Metrikleri kaydet\n",
    "with open(\"/content/drive/MyDrive/train_metrics_base.json\", \"w\") as f:\n",
    "    json.dump(train_output.metrics, f, indent=4)\n",
    "\n",
    "eval_metrics = trainer.evaluate()\n",
    "with open(\"/content/drive/MyDrive/eval_metrics_base.json\", \"w\") as f:\n",
    "    json.dump(eval_metrics, f, indent=4)\n",
    "\n",
    "# 11. Model ve processor'ü kaydet\n",
    "trainer.save_model(\"/content/drive/MyDrive/convnext-model-base\")\n",
    "processor.save_pretrained(\"/content/drive/MyDrive/convnext-model-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 233298,
     "status": "ok",
     "timestamp": 1752912301118,
     "user": {
      "displayName": "ZEYNEP YARDIMCI",
      "userId": "08243208065676408191"
     },
     "user_tz": -180
    },
    "id": "7oddVx2UovCB",
    "outputId": "5e39c148-14bb-4a81-fcff-09fd2ed5536a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.6.0+cu118\n",
      "Uninstalling torch-2.6.0+cu118:\n",
      "  Successfully uninstalled torch-2.6.0+cu118\n",
      "Found existing installation: torchvision 0.21.0+cu124\n",
      "Uninstalling torchvision-0.21.0+cu124:\n",
      "  Successfully uninstalled torchvision-0.21.0+cu124\n",
      "Found existing installation: torchaudio 2.6.0+cu124\n",
      "Uninstalling torchaudio-2.6.0+cu124:\n",
      "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "Files removed: 38\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m905.3/905.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sympy-1.13.3 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "566d53d88d4b4375ac1fcefe54cda084",
       "pip_warning": {
        "packages": [
         "sympy",
         "torch",
         "torchgen",
         "triton"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip cache purge\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11983,
     "status": "ok",
     "timestamp": 1752912370542,
     "user": {
      "displayName": "ZEYNEP YARDIMCI",
      "userId": "08243208065676408191"
     },
     "user_tz": -180
    },
    "id": "vYVndFYXs5tF",
    "outputId": "c3f1c067-9ed6-43f5-b1b8-601b9f22357b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.53.2 in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.53.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1752912495990,
     "user": {
      "displayName": "ZEYNEP YARDIMCI",
      "userId": "08243208065676408191"
     },
     "user_tz": -180
    },
    "id": "3IFr1z6Zs-2o",
    "outputId": "0d5564e6-eef8-47ba-dff7-5a4b9f23c9d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print(inspect.signature(TrainingArguments.__init__))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHLgz_d2WOij"
   },
   "source": [
    "Model Değerlendirme ve Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 58599,
     "status": "ok",
     "timestamp": 1752934818722,
     "user": {
      "displayName": "ZEYNEP YARDIMCI",
      "userId": "08243208065676408191"
     },
     "user_tz": -180
    },
    "id": "VqrtxcdGPEzt",
    "outputId": "65d7bf8d-36be-4fde-e476-ee4b3c0bff50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/convnext/feature_extraction_convnext.py:30: FutureWarning: The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      " 1. Potassium Deficiency     0.8800    1.0000    0.9362        22\n",
      " 2. Manganese Deficiency     1.0000    1.0000    1.0000         1\n",
      " 3. Magnesium Deficiency     0.9000    1.0000    0.9474         9\n",
      "         4. Black Scorch     0.8667    0.9286    0.8966        14\n",
      "           5. Leaf Spots     0.9333    1.0000    0.9655        14\n",
      "        6. Fusarium Wilt     0.9091    0.9091    0.9091        33\n",
      "        7. Rachis Blight     1.0000    0.8710    0.9310        31\n",
      "8. Parlatoria Blanchardi     0.9286    0.9286    0.9286        14\n",
      "       9. Healthy sample     1.0000    0.8462    0.9167        13\n",
      "\n",
      "                accuracy                         0.9272       151\n",
      "               macro avg     0.9353    0.9426    0.9368       151\n",
      "            weighted avg     0.9315    0.9272    0.9270       151\n",
      "\n",
      "Accuracy: 0.9272\n",
      "\n",
      "\n",
      "✅ Değerlendirme raporu başarıyla kaydedildi: /content/drive/MyDrive/model_sonuclari3/degerlendirme_raporu3.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, AutoFeatureExtractor, ConvNextForImageClassification\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from google.colab import drive\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# WandB'yi kapatmak için:\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# 1. Google Drive'ı mount et\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# 2. Model ve Processor'ü yükle\n",
    "model_path = \"/content/drive/MyDrive/convnext-model3\"\n",
    "model = ConvNextForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoFeatureExtractor.from_pretrained(model_path)\n",
    "\n",
    "# 3. Test dataset (Raw klasöründeki veriyi tekrar yükle ve split)\n",
    "data_dir = \"/content/drive/MyDrive/Raw\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(root=data_dir, transform=transform)\n",
    "_, test_size = int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))\n",
    "_, test_ds = torch.utils.data.random_split(dataset, [int(0.8 * len(dataset)), test_size])\n",
    "\n",
    "# 4. Collate function (modelin beklentisine göre)\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "    labels = torch.tensor(labels)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# 5. Trainer ayarları\n",
    "from transformers import TrainingArguments\n",
    "args = TrainingArguments(output_dir=\"./test-eval\", per_device_eval_batch_size=8)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "# 6. Test tahmini\n",
    "outputs = trainer.predict(test_ds)\n",
    "y_pred = np.argmax(outputs.predictions, axis=1)\n",
    "y_true = outputs.label_ids\n",
    "\n",
    "# 7. Sınıf isimleri\n",
    "label_names = dataset.classes  \n",
    "\n",
    "# 8. Raporu oluştur\n",
    "active_labels = unique_labels(y_true, y_pred)\n",
    "active_label_names = [label_names[i] for i in active_labels]\n",
    "\n",
    "report = classification_report(y_true, y_pred, labels=active_labels, target_names=active_label_names, digits=4)\n",
    "# Accuracy hesapla\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Raporun sonuna accuracy'yi ekle\n",
    "report_with_accuracy = report + f\"\\nAccuracy: {accuracy:.4f}\\n\"\n",
    "\n",
    "print(report_with_accuracy)\n",
    "\n",
    "# 9. Google Drive'a kaydet\n",
    "save_path = \"/content/drive/MyDrive/model_sonuclari3/degerlendirme_raporu3.txt\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report_with_accuracy)\n",
    "\n",
    "print(f\"\\nDeğerlendirme raporu başarıyla kaydedildi: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP3lu6StEkJCXiXVFTI2HoI",
   "gpuType": "T4",
   "mount_file_id": "1dEy-U1jO0wfS602wGI-7fQrEZY78_pKG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
